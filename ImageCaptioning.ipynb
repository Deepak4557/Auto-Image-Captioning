{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_Dataset.zip\n",
        "!wget https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_text.zip"
      ],
      "metadata": {
        "id": "_ymFE6Dp0MVH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip Flickr8k_Dataset.zip -d all_images\n",
        "!unzip Flickr8k_text.zip -d all_captions"
      ],
      "metadata": {
        "id": "HgKeMUXj0TiF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#importing the libraries\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow\n",
        "from tqdm import tqdm\n",
        "from keras.applications import vgg16\n",
        "from keras.preprocessing import image\n",
        "from keras.applications.vgg16 import preprocess_input\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50\n",
        "from keras.models import Model\n",
        "from keras.preprocessing.text import one_hot\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from numpy import array\n",
        "import pickle\n",
        "import numpy as np\n",
        "import os\n",
        "import cv2"
      ],
      "metadata": {
        "id": "L-PJ9bMm0VKr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#converting the text files to pandas dataframe\n",
        "image_tokens=pd.read_csv(\"all_captions/Flickr8k.lemma.token.txt\",sep='\\t',names=[\"img_id\",\"img_caption\"])\n",
        "train_image_names=pd.read_csv(\"all_captions/Flickr_8k.trainImages.txt\",names=[\"img_id\"])\n",
        "test_image_names=pd.read_csv(\"all_captions/Flickr_8k.testImages.txt\",names=[\"img_id\"])\n",
        "val_image_names=pd.read_csv(\"all_captions/Flickr_8k.devImages.txt\",names=[\"img_id\"])"
      ],
      "metadata": {
        "id": "AABzfRn20h8S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#just checking the number of images in the dataset\n",
        "l=os.listdir(\"all_images/Flicker8k_Dataset\")\n",
        "print(len(l))"
      ],
      "metadata": {
        "id": "9_iNNuUK0klM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#visualising one of the train images\n",
        "img=cv2.imread(\"all_images/Flicker8k_Dataset\"+ \"/\" +train_image_names.img_id[2])\n",
        "plt.imshow(cv2.cvtColor(img,cv2.COLOR_BGR2RGB))"
      ],
      "metadata": {
        "id": "aPvDst4K0nXs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#removing the #0,#1,#2,#3,#5 from the image ids\n",
        "image_tokens[\"img_id\"]=image_tokens[\"img_id\"].map(lambda x: x[:len(x)-2])"
      ],
      "metadata": {
        "id": "KNXmEf5h0qT9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_tokens[\"img_caption\"]=image_tokens[\"img_caption\"].map(lambda x: \"<start> \" + x.strip() + \" <end>\")"
      ],
      "metadata": {
        "id": "koUNZrc1022u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#head of the image_tokens dataframe\n",
        "image_tokens.head()"
      ],
      "metadata": {
        "id": "hlJQ99dK03Zm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#head of the train_image_names dataframe\n",
        "train_image_names.head()"
      ],
      "metadata": {
        "id": "O_JyH17m06CQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating train dictionary having key as the image id and value as a list of its captions\n",
        "train_captions={}\n",
        "for i in tqdm(range(len(train_image_names))):\n",
        "  l=[caption for caption in(image_tokens[image_tokens[\"img_id\"]==train_image_names[\"img_id\"].iloc[i]].img_caption)]\n",
        "  train_captions[train_image_names[\"img_id\"].iloc[i]]=l"
      ],
      "metadata": {
        "id": "Sfq1JNCs09xx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating test dictionary having key as the image id and value as a list of its captions\n",
        "test_captions={}\n",
        "for i in tqdm(range(len(test_image_names))):\n",
        "  l=[caption for caption in(image_tokens[image_tokens[\"img_id\"]==test_image_names[\"img_id\"].iloc[i]].img_caption)]\n",
        "  test_captions[test_image_names[\"img_id\"].iloc[i]]=l"
      ],
      "metadata": {
        "id": "9T4g49ue1H-0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating validation dictionary having key as the image id and value as a list of its captions\n",
        "validation_captions={}\n",
        "for i in tqdm(range(len(val_image_names))):\n",
        "  l=[caption for caption in(image_tokens[image_tokens[\"img_id\"]==val_image_names[\"img_id\"].iloc[i]].img_caption)]\n",
        "  validation_captions[val_image_names[\"img_id\"].iloc[i]]=l"
      ],
      "metadata": {
        "id": "sHoWxggx1RVb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=ResNet50(include_top=False, weights='imagenet',pooling='avg',input_shape=(224,224,3))\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "5ie8GNrO1U3U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#extracting image encodings(features) from resnet50 and forming dict train_features\n",
        "path=\"all_images/Flicker8k_Dataset/\"\n",
        "train_features={}\n",
        "c=0\n",
        "for image_name in tqdm(train_captions):\n",
        "  img_path=path+image_name\n",
        "  img=image.load_img(img_path,target_size=(224,224))\n",
        "  x = image.img_to_array(img)\n",
        "  x = np.expand_dims(x, axis=0)\n",
        "  x = preprocess_input(x)\n",
        "  features = model.predict(x)\n",
        "  train_features[image_name]=features.squeeze()"
      ],
      "metadata": {
        "id": "iJeLsC2S1XHq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open( \"train_encoded_images.p\", \"wb\" ) as pickle_f:\n",
        "    pickle.dump(train_features, pickle_f )"
      ],
      "metadata": {
        "id": "9qpU1PiO_Edm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " #extracting image encodings(features) from resnet50 and forming dict test_features\n",
        "path=\"all_images/Flicker8k_Dataset/\"\n",
        "test_features={}\n",
        "c=0\n",
        "for image_name in tqdm(test_captions):\n",
        "  img_path=path+image_name\n",
        "  img=image.load_img(img_path,target_size=(224,224))\n",
        "  x = image.img_to_array(img)\n",
        "  x = np.expand_dims(x, axis=0)\n",
        "  x = preprocess_input(x)\n",
        "  features = model.predict(x)\n",
        "  test_features[image_name]=features.squeeze()"
      ],
      "metadata": {
        "id": "FNavbfRD_H-G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open( \"test_encoded_images.p\", \"wb\" ) as pickle_f:\n",
        "    pickle.dump(test_features, pickle_f )"
      ],
      "metadata": {
        "id": "XNT28kbyAxS8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#extracting image encodings(features) from resnet50 and forming dict validation_features\n",
        "path=\"all_images/Flicker8k_Dataset/\"\n",
        "validation_features={}\n",
        "c=0\n",
        "for image_name in tqdm(validation_captions):\n",
        "  img_path=path+image_name\n",
        "  img=image.load_img(img_path,target_size=(224,224))\n",
        "  x = image.img_to_array(img)\n",
        "  x = np.expand_dims(x, axis=0)\n",
        "  x = preprocess_input(x)\n",
        "  features = model.predict(x)\n",
        "  validation_features[image_name]=features.squeeze()"
      ],
      "metadata": {
        "id": "C1gslCV0A1LD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open( \"validation_encoded_images.p\", \"wb\" ) as pickle_f:\n",
        "    pickle.dump(validation_features, pickle_f )"
      ],
      "metadata": {
        "id": "ExY-tACRCaZR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_captions=[]\n",
        "for img_id in tqdm(train_captions):\n",
        "  for captions in train_captions[img_id]:\n",
        "    all_captions.append(captions)\n",
        "\n",
        "all_words=\" \".join(all_captions)\n",
        "print()\n",
        "print(len(all_words))\n",
        "unique_words=list(set(all_words.strip().split(\" \")))\n",
        "print(len(unique_words))"
      ],
      "metadata": {
        "id": "lIOFpDUICdsP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#defining max_length and vocabulary size\n",
        "vocab_size=len(unique_words)+1\n",
        "max_length=40"
      ],
      "metadata": {
        "id": "W6gyRSXRChhm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#forming dictionaries containg mapping of words to indices and indices to words\n",
        "words_to_indices={val:index+1 for index, val in enumerate(unique_words)}\n",
        "indices_to_words = { index+1:val for index, val in enumerate(unique_words)}\n",
        "words_to_indices[\"Unk\"]=0\n",
        "indices_to_words[0]=\"Unk\""
      ],
      "metadata": {
        "id": "7QidU3DQCksD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#forming dictionary having encoded captions\n",
        "train_encoded_captions={}\n",
        "for img_id in tqdm(train_captions):\n",
        "  train_encoded_captions[img_id]=[]\n",
        "  for i in range(5):\n",
        "    train_encoded_captions[img_id].append([words_to_indices[s] for s in train_captions[img_id][i].split(\" \")])"
      ],
      "metadata": {
        "id": "3jNoXE3gCohd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for img_id in tqdm(train_encoded_captions):\n",
        "  print(train_encoded_captions[img_id])\n",
        "  break"
      ],
      "metadata": {
        "id": "r9oq2TtkCrSY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for img_id in tqdm(train_encoded_captions):\n",
        "  train_encoded_captions[img_id]=pad_sequences(train_encoded_captions[img_id], maxlen=max_length, padding='post')"
      ],
      "metadata": {
        "id": "y6mnRdmzCxFM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for img_id in tqdm(train_encoded_captions):\n",
        "  print(train_encoded_captions[img_id])\n",
        "  break"
      ],
      "metadata": {
        "id": "88vGWLA2C7LK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for x in train_encoded_captions['2513260012_03d33305cf.jpg'][2]:\n",
        "  print(indices_to_words[x])"
      ],
      "metadata": {
        "id": "cKNvZhjlC_K_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_encoded_captions[\"2513260012_03d33305cf.jpg\"][0][0:1].tolist()"
      ],
      "metadata": {
        "id": "odmSb9dTDBb4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def data_generator(train_encoded_captions,train_features,num_of_photos):\n",
        "  X1, X2, Y = list(), list(), list()\n",
        "  max_length=40\n",
        "  n=0\n",
        "  for img_id in tqdm(train_encoded_captions):\n",
        "    n+=1\n",
        "    for i in range(5):\n",
        "      for j in range(1,40):\n",
        "        curr_sequence=train_encoded_captions[img_id][i][0:j].tolist()\n",
        "        next_word=train_encoded_captions[img_id][i][j]\n",
        "        curr_sequence=pad_sequences([curr_sequence], maxlen=max_length, padding='post')[0]\n",
        "        one_hot_next_word=to_categorical([next_word],vocab_size)[0]\n",
        "        X1.append(train_features[img_id])\n",
        "        X2.append(curr_sequence)\n",
        "        Y.append(one_hot_next_word)\n",
        "    if(n==num_of_photos):\n",
        "      yield [[array(X1), array(X2)], array(Y)]\n",
        "      X1, X2, Y = list(), list(), list()\n",
        "      n=0"
      ],
      "metadata": {
        "id": "sGc2Iuu5DFyA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#importing dependencies\n",
        "from keras.models import Model\n",
        "from keras.layers import Input\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import add\n",
        "from keras.layers import Embedding"
      ],
      "metadata": {
        "id": "HdABga40EFDX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in train_features:\n",
        "  print(train_features[i].shape)\n",
        "  break"
      ],
      "metadata": {
        "id": "PiM3AszJENKp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#model\n",
        "input_1=Input(shape=(2048,))\n",
        "dropout_1=Dropout(0.2)(input_1)\n",
        "dense_1=Dense(256,activation='relu')(dropout_1)\n",
        "\n",
        "input_2=Input(shape=(max_length,))\n",
        "embedding_1=Embedding(vocab_size,256)(input_2)\n",
        "dropout_2=Dropout(0.2)(embedding_1)\n",
        "lstm_1=LSTM(256)(dropout_2)\n",
        "\n",
        "add_1=add([dense_1,lstm_1])\n",
        "dense_2=Dense(256,activation='relu')(add_1)\n",
        "dense_3=Dense(vocab_size,activation='softmax')(dense_2)\n",
        "\n",
        "model=Model(inputs=[input_1,input_2],outputs=dense_3)\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "UMmfJ8ltEP9Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "uxDvwv2WETmK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_encoded_captions)"
      ],
      "metadata": {
        "id": "_F8_Q7VFEY9q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs=1\n",
        "no_of_photos=5\n",
        "steps=len(train_encoded_captions)//no_of_photos\n",
        "for i in range(epochs):\n",
        "  generator=data_generator(train_encoded_captions,train_features,no_of_photos)\n",
        "  model.fit(generator,epochs=1,steps_per_epoch=steps,verbose=1)"
      ],
      "metadata": {
        "id": "i5DkHDZJxSkj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_features"
      ],
      "metadata": {
        "id": "BoS_-ADOxS8S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def beam_search(photo,k):\n",
        "  photo=photo.reshape(1,2048)\n",
        "  in_text='<start>'\n",
        "  sequence = [words_to_indices[s] for s in in_text.split(\" \") if s in words_to_indices]\n",
        "  sequence = pad_sequences([sequence], maxlen=max_length, padding='post')\n",
        "  y_pred = model.predict([photo,sequence],verbose=0)\n",
        "  predicted=[]\n",
        "  y_pred=y_pred.reshape(-1)\n",
        "  for i in range(y_pred.shape[0]):\n",
        "    predicted.append((i,y_pred[i]))\n",
        "  predicted=sorted(predicted,key=lambda x:x[1])[::-1]\n",
        "  b_search=[]\n",
        "  for i in range(k):\n",
        "    word = indices_to_words[predicted[i][0]]\n",
        "    b_search.append((in_text +' ' + word,predicted[i][1]))\n",
        "\n",
        "  for idx in range(max_length):\n",
        "    b_search_square=[]\n",
        "    for text in b_search:\n",
        "      if text[0].split(\" \")[-1]==\"<end>\":\n",
        "        break\n",
        "      sequence = [words_to_indices[s] for s in text[0].split(\" \") if s in words_to_indices]\n",
        "      sequence = pad_sequences([sequence], maxlen=max_length, padding='post')\n",
        "      y_pred = model.predict([photo,sequence],verbose=0)\n",
        "      predicted=[]\n",
        "      y_pred=y_pred.reshape(-1)\n",
        "      for i in range(y_pred.shape[0]):\n",
        "        predicted.append((i,y_pred[i]))\n",
        "      predicted=sorted(predicted,key=lambda x:x[1])[::-1]\n",
        "      for i in range(k):\n",
        "        word = indices_to_words[predicted[i][0]]\n",
        "        b_search_square.append((text[0] +' ' + word,predicted[i][1]*text[1]))\n",
        "    if(len(b_search_square)>0):\n",
        "      b_search=(sorted(b_search_square,key=lambda x:x[1])[::-1])[:5]\n",
        "  final=b_search[0][0].split()\n",
        "  final = final[1:-1]\n",
        "  #final=\" \".join(final)\n",
        "  return final"
      ],
      "metadata": {
        "id": "TW2EY-Q2MuAU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "i=0\n",
        "for img_id in test_features:\n",
        "  i+=1\n",
        "  img=cv2.imread(\"all_images/Flicker8k_Dataset\"+ \"/\" + img_id)\n",
        "  plt.imshow(cv2.cvtColor(img,cv2.COLOR_BGR2RGB))\n",
        "  photo=test_features[img_id]\n",
        "  plt.show()\n",
        "  reference=[]\n",
        "  for caps in test_captions[img_id]:\n",
        "    list_caps=caps.split(\" \")\n",
        "    list_caps=list_caps[1:-1]\n",
        "    reference.append(list_caps)\n",
        "  candidate=beam_search(photo,3)\n",
        "  print(\"Predicted Captions: \")\n",
        "  for cap in reference:\n",
        "    print(\" \".join(cap))\n",
        "    break\n",
        "  if(i == 5):\n",
        "    break"
      ],
      "metadata": {
        "id": "FGUZWS1aMw0W"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}